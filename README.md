# RoBERTa_Sentiment_Analysis

## Transformer-based Models for Low Resource Languages

### Need for low-resource language:
- Bridging the digital divide and make information accessible to people speaking low resource languages
- Preserving linguistic diversity by documenting and preserving these languages for future generations and maintain cultural heritage
- Enabling machine learning in local contexts leading to more effective and culturally sensitive applications

Three models were compared across each other for performance and then evaluated across similar datasets.

### Datasets used:
1. Hi_3500 dataset - movie review
2. IIT Patna - Movie review dataset
3. IIT Patna - Product review dataset


### Prediction information - wrong classification
<img width="654" alt="image" src="https://user-images.githubusercontent.com/60126568/233812161-6d15ec36-8f43-4405-ab68-f055e6f13694.png">


### Performance Evaluation:
<img width="674" alt="image" src="https://user-images.githubusercontent.com/60126568/233812169-97241755-fd79-4d4b-9303-fc19292d526b.png">

### Reference / Source: 
https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment,
https://huggingface.co/monsoon-nlp/hindi-tpu-electra
https://huggingface.co/AshiNLP/Hindi-RoBERTa/tree/main
